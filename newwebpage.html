<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
   <head>
        <title>Tarek El-Gaaly</title> <!-- change to whatever you want as the title for your website -->
        <meta name="description" content=" " /> <!-- enter a description for your website inside the " " -->
        <meta name="keywords" content=" " /> <!-- enter a a string of keywords that relate to your website inside the " " -->
        <meta http-equiv="Content-Language" content="en-gb" />
        <meta http-equiv="Content-Type" content="text/html; charset=windows-1252" />
        <link rel="stylesheet" type="text/css" href="newwebpage.css" />
        
        <style type="text/css">
            td { margin: 100px 0px; } /* tells the table to have 25
                                      px on top and bottom, zero px on left and right */
        </style>
    </head>
    <body  width="100%">
        <div id="page-wrap" align="left" style="margin:40px;font-size: 18pt;"  width="100%">
        
    <table id="page-wrap" align="left" border=0 margin-left=100px a width="100%" border="0" style="margin:40px auto; font-size: 16pt; line-height: 100%;" BORDER=1  CELLPADDING=3 CELLSPACING=1>
    
    <tr>
        <td>
            <img alt="image description" src="images2/prof.jpg" width="260" height="380" />
        </td>
        <td>
            <p style="font-size: 24pt; line-height: 100%;"><b>Tarek El-Gaaly</b></p>
            <p>PhD Candidate @ Department of Computer Science, Rutgers University</p>
            <p>Contact: tgaaly [at] rutgers [dot] edu</p>
            <br/>
            <b>Short Bio:</b> I am a computer science Ph.D. student at Rutgers University. I am a member of the Computer Vision Group at CBIM under Professor <a title="About Me" href="http://www.cs.rutgers.edu/~elgammal/Home.html">Ahmed Elgammal</a>. My research interests are in Computer Vision, Machine Learning and Robotics. more information can be found in my <a href="Tarek.ElGaaly.RutgersUniversity.resume.pdf">resume</a>.
            </p>
        </td>
    </tr>
    </table>

    <br>
    <b>Publications / Projects</b>
    
<table id="page-wrap" align="left" border=0 margin-left=100px width="100%" border="0" style="margin:20px  auto; font-size: 12pt" BORDER=1 CELLPADDING=3 CELLSPACING=1
    RULES=ROWS FRAME=HSIDES>
        
		<tr>
			<td> <img src="3DObjecDecomp2.png" width="480" height="280" /> <td/>
			<td> <b>Perceptually-Grounded Probabilistic Object-Part Decomposition of 3D Point Clouds</b>
                </br>
                This work uses Bayesian Hierarchical Grouping for perceptual object-part decomposition based on medial-axis representations of parts.
                
                <ul>
                    
                    
                    <li><b>Tarek El-Gaaly</b>, Vicky Froyen, Ahmed Elgammal, Jacob Feldman and Manish Singh, <b><u>A Bayesian Approach to Perceptual 3D Object-Part Decomposition using Skeleton-based Representations</u></b>, Accepted AAAI 2015 <font color="red">(26.67% acceptance rate) </font>
                        <a href="aaai15_paper.pdf" style="color:#800000">[pdf]</a>
                    </li>
                    
                </ul>
            </td>
		</tr>
        <tr>
			<td> <img src="joint.jpg" width="480" height="280" />
                <br/>
                <img src="joint2.jpg" width="480" height="420" />
                <iframe width="480" height="315" src="http://www.youtube.com/embed/IzaWJTiGmww" frameborder="0" allowfullscreen></iframe><td/>
                
                <td>
                    <b>Joint Object Categorization and Pose Estimation</b>
                    </br>
                    Using manifold analysis to perform joint object categorization, instance recognition and pose estimation. Multiiple images of an object are known to lie on a low-dimensional view-manifolds. The premise of this work is that feature spaces deform unit circle view-manifolds, in the case of table-top objects rotating on a turn-table and captured by a camera from a fixed height, or a sphere manifold in the more general case. The deformation is captured by a homeomorphic mapping from input feature space to points on a conceptual view-manifold (which can be seen to represent the low-dimensional geometry of views around an object).
                    
                    <ul>
                        
                        <li>Haopeng Zhang, <b>Tarek El-Gaaly</b>, Ahmed Elgammal, Zhiguo Jiang <b><u>Factorization of View-Object Manifolds for Joint
                            Object Recognition and Pose Estimation</u></b>, ElSevier - Computer Vision and Image Understanding (CVIU) 2015 <a href="http://arxiv.org/pdf/1503.06813.pdf" style="color:#800000">[pdf]</a>
                        </li>
                        <li>Haopeng Zhang, <b>Tarek El-Gaaly</b>, Ahmed Elgammal, Zhiguo Jiang <b><u>Joint Object and Pose Recognition using Homeomorphic Manifold Analysis</u></b>, AAAI 2013 <font color="red">(29% acceptance rate) </font>
                            <a href="aaai_paper.pdf" style="color:#800000">[pdf]</a>
                        </li>
                        
                    </ul>
                <td/>
                <td></td>

        </tr>
        
        
        <tr>
			<td> <img src="rgbd12.jpg" width="480" height="260" />
                <img src="rgbd2.jpg" width="480" height="170" />
                <td/>
                <td> <b>RGBD Table-top Object Pose Recognition</b><br/>
                    The red annotations are the ground-truth pose angles (i.e. azimuth/yaw) of the tabletop objects
                    (from the RGBD-dataset - University of Washington). Blue annotations signify the estimated pose based on visual local feature information alone. Green annotations represent the final recognized pose using both visual and depth information [Refer to ICPR 2012].
                    <ul>
                    <li><b>Tarek El-Gaaly</b>, Haopeng Zhang, Marwan Torki, Ahmed Elgammal, Maneesh Singh and Zhiguo Jiang
                        
                        <b><u>Multi-Modal RGBD Sensors for Object Recognition</u></b>, Special Session on RGBD Applications, Asian Conference on Computer Vision, ACCV 2012
                    </li>
                    <li><b>Tarek El-Gaaly</b>, Marwan Torki, Ahmed Elgammal, Maneesh Singh,
                        
                        <b><u>Multi-Modal RGBD Sensors for Object Grasping and Manipulation</u></b>, Workshop: "Beyond Grasping: Modern Approaches for Dextrous Manipulation",
                        
                        Intelligent Robots and Systems, IROS 2012
                    </li>
                    
                    
                    
                    <li><b>Tarek El-Gaaly</b>, Marwan Torki, Ahmed Elgammal, Maneesh Singh, <b><u>RGBD Object Pose Recognition using Local-Global
                        
                        Multi-Kernel Regression</u></b>, International Conference on Pattern Recognition, ICPR 2012 <a href="icpr_paper.pdf" style="color:#800000">[pdf]</a>
                    </li>
                    </ul>

                    <td/>
                    <td></td>
        </tr>
        
        <tr>
			<td> <img src="images2/local_features3.png" width="480" height="280" /><br/>
                <img src="images2/local_features4.png" width="480" height="280" />
                <td/>
                <td> <b>Object Localization using Label Propagation over Local Features</b><br/>
                    <td/>
                    <td></td>
                    </tr>
        
        <tr>
			<td> <img src="images2/srr.jpeg" width="480" height="280" /> <td/>
                <td> <b>NASA Centennial Challenge 2013 - Sample Return Challenge</b><br/>
                    Collaborated with Worcester Polytechnic Institute (WPI) on this challenge.
                    Our robot AERO (Autonomous Exploration Rover) can be seen below on the starting platform with us in the background.
                    The website documenting the building of the robot can be seen here:
                    <td/>
                    <td></td>
        </tr>
        
      <!--
        <tr>
			<td> <img src="images2/srr.jpeg" width="480" height="280" /> <td/>
                <td> <b>NASA Centennial Challenge 2013 - Sample Return Challenge</b><br/>
                    Collaborated with Worcester Polytechnic Institute (WPI) on this challenge.
                    Our robot AERO (Autonomous Exploration Rover) can be seen below on the starting platform with us in the background.
                    The website documenting the building of the robot can be seen here:
                    <td/>
                    <td></td>
        </tr>
        
        <tr>
			<td> <img src="images2/srr.jpeg" width="480" height="280" /> <td/>
                <td> <b>NASA Centennial Challenge 2013 - Sample Return Challenge</b><br/>
                    Collaborated with Worcester Polytechnic Institute (WPI) on this challenge.
                    Our robot AERO (Autonomous Exploration Rover) can be seen below on the starting platform with us in the background.
                    The website documenting the building of the robot can be seen here:
                <td/>
                <td></td>
        </tr>
       -->


        <tr>
			<td> <iframe width="480" height="315" src="http://www.youtube.com/embed/sVeYu3NA8KM" frameborder="0" allowfullscreen></iframe> <td/>
                <td> <b>Autonomous Airboat Obstacle Avoidance (using monocular vision on a smartphone)</b>
                    </br>Work was conducted at Carnegie Mellon University - Robotics Institute (Field Robotics Center) <a href="http://code.google.com/p/crw-cmu/" target="_blank">(CMU Cooperative Robotic Watercraft)</a>
                    <p></p>
                    More videos on <a href="http://robots.net/article/3445.html" target="_blank">Robotics.net</a>
                <td/>
                <td></td>
        </tr>
        
        <tr>
			<td> <img src="pollution_spectrum.png" width="480" height="140" /><br/>
                <!--<img src="dehazing.png" width="280" height="140" />-->
                <img src="pol_dc_co_comparison.png" width="480" height="140" /><br/> <td/>
                <td> <p>The first figure shows a sequence of images of a hazy scene. The second figure shows the scene resulting from state-of-the-art dehazing and our dehazing algorithm. The last figure shows a comparison with 2 other techniques for dehazing: polarization dehazing and dark channel dehazing. Our dehazing method recovers the hue of the scene and also returns a natural looking sky without any extra processing (see rightmost image of the second figure). Refer to VISAPP 2010 paper below. </p>
                    <p>For a full description of my MSc Thesis - refer to the thesis document:
                    <a href="Tarek El-Gaaly - Final Thesis Document.pdf" style="color:#800000">[pdf]</a>
                <td/>
                <td></td>
        </tr>
        
	</table>
    
    </div>

	</body>