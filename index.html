<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
    <!-- this template was designed by http://www.tristarwebdesign.co.uk - please visit for more templates & information - thank you. -->
    <head>
        <title>Tarek El-Gaaly</title> <!-- change to whatever you want as the title for your website -->
        <meta name="description" content=" " /> <!-- enter a description for your website inside the " " -->
        <meta name="keywords" content=" " /> <!-- enter a a string of keywords that relate to your website inside the " " -->
        <meta http-equiv="Content-Language" content="en-gb" />
        <meta http-equiv="Content-Type" content="text/html; charset=windows-1252" />
        <link rel="stylesheet" type="text/css" href="css/style.css" />
    </head>
    <body>
        
        <div id="container">
            
            <div id="headercont">
                <div>
                    <br>
                    <p line-height: 120%;><font size="13" color="black">Tarek El-Gaaly</font></p>
                    <p line-height: 120%;><font size="3" color="#d9153b">Ph.D. Candidate, Rugers University</font></p>
                </div>
                <div id="menu">
                    <ul>
                        <li><a title="home" href="index.html">home</a></li>
                        <li><a title="Aerial Vehicle Research" href="mav.html">Aerial Vehicle research</a></li>
                        <li><a title="Publications" href="pubs.html">Publications</a></li>
                        <li><a title="Teaching" href="teaching.html">Teaching</a></li>
                        <li><a title="Funds/Grants" href="funds_grants.html">Funds/GRants</a></li>
                        <li><a title="Vision Reading Group" href="vision_group.html">vision group</a></li>
                        <li><a title="Code" href="code.html">Code</a></li>
                        <li><a title="About Me" href="aboutme.html">About Me</a></li>
                    </ul>
                </div>
            </div>
            
            <!--<div>-->
                <!--<img class="imgleft" alt="image description" src="images2/header.jpg" width="850"/>-->
                <!--<p>"Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Vivamus pulvinar purus nec nibh. Proin a erat. In molestie nisi id mauris. Cras leo. Vivamus dapibus nisi non est. Curabitur libero" - <span>Cras quam metus</span></p>
                 --><!--</div>-->
            
            <div id="maincont">
                <div id="mainleft">
                    
                    
                    
                    <p></p>
                    <b>Short Bio:</b> I am a computer science Ph.D. student at Rutgers University. I am a member of the Computer Vision Group at the Center for Biomedical Imaging and Modelling (<a title="CBIM" href="http://cbim.rutgers.edu">CBIM</a>) under Professor <a title="CBIM" href="https://www.cs.rutgers.edu/~dnm/">Dimitri Metaxas</a>. My advisor is Professor <a title="My Advisor" href="http://www.cs.rutgers.edu/~elgammal/Home.html">Ahmed Elgammal</a>. My research focus is Computer Vision, Machine Learning and Robotics. More information can be found in my <a href="Tarek.ElGaaly.RutgersUniversity.resume.pdf">resume</a>.
                    <p></p><p></p><p></p>
                    
                    <br/>
                    <br/>
                    
                    <table id="page-wrap" align="left" border=0 margin-left=100px width="100%" border="1" style="margin:20px  auto; font-size: 9pt ; padding:0 25px 0 25px; border-spacing: 50px 0;" BORDER=1 CELLPADDING=3 CELLSPACING=3 RULES=ROWS FRAME=HSIDES>
                        
                        <h1>Selected Projects</h1>
                        
                        <tr>
                            <td><img src="dl.jpg" width="280" height="180" /></td>
                            <td>
                                <b>[More coming soon...] Object Recognition using Convolutional Neural Networks</b></br> We analyze the layers of CNNs to see what happens to object-view manifolds. More coming soon.
                                </br></br>
                                Amr Bakry*, Mohamed Elhoseiny*, <b>Tarek El-Gaaly*</b> and Ahmed Elgammal <b><u>Digging Deep into the Layers of CNNs: In Search of How CNNs Achieve View Invariance</u></b> (*equal contribution) - <a href="http://arxiv.org/abs/1508.01983" style="color:#800000">[pdf]</a>
                            </td>
                        </tr>
                        <tr>
                            <td><img src="3DObjecDecomp2.png" width="280" height="180" /></td>
                            <td>
                                <b>3D Object Recognition:</b></br> We built a Bayesian hierarchical grouping model for perceptual object-part decomposition based on medial-axis representations of parts.
                                </br>
                                </br>
                                This work was published in the Association for the Advancement of Artificial Intelligence (AAAI) 2015:
                                </br></br>

                                    <b>Tarek El-Gaaly</b>, Vicky Froyen, Ahmed Elgammal, Jacob Feldman and Manish Singh, <b><u>A Bayesian Approach to Perceptual 3D Object-Part Decomposition using Skeleton-based Representations</u></b>, Accepted AAAI 2015 <font color="red">(~26% acceptance rate)</font> <a href="aaai15_paper.pdf" style="color:#800000">[pdf]</a> <a href="http://dblp.uni-trier.de/rec/bibtex/conf/aaai/El-GaalyFEFS15" style="color:#800000">[bibtex]</a>
                                
                            </td>
                        </tr>
                        <tr>
                            <td><img src="joint.jpg" width="280" height="200" /><br/>
                                <img src="joint2.jpg" width="280" height="220" /></br>
                            <iframe width="280" height="180" src="http://www.youtube.com/embed/IzaWJTiGmww" allowfullscreen></iframe></td>
                            <td>
                                <b>Joint object Categorization and Pose Estimation:</b></br> In this work we build a framework based on object-view manifold analysis to perform simultaneous object categorization, instance recognition and pose estimation. Multiiple images of an object are known to lie on a low-dimensional intrinsic view-manifold. The premise of this work is that feature spaces deform unit circle view-manifolds, in the case of table-top objects rotating on a turn-table and captured by a camera from a fixed height, or a sphere manifold in the more general case. The deformation is captured by a homeomorphic mapping from the input feature space to points on this conceptual view-manifold. I also built a near real-time system based on this work (see video on left).
                                
                                    </br></br>
                                    This work was published in: </br>
                                    <li>Haopeng Zhang, <b>Tarek El-Gaaly</b>, Ahmed Elgammal, Zhiguo Jiang <b><u>Factorization of View-Object Manifolds for Joint
                                    Object Recognition and Pose Estimation</u></b>, ElSevier - Computer Vision and Image Understanding (CVIU) 2015 <a href="http://arxiv.org/pdf/1503.06813.pdf" style="color:#800000">[pdf]</a>
                                    </li>
                                    <li>Haopeng Zhang, <b>Tarek El-Gaaly</b>, Ahmed Elgammal, Zhiguo Jiang <b><u>Joint Object and Pose Recognition using Homeomorphic Manifold Analysis</u></b>, AAAI 2013 <font color="red">(~29% acceptance rate) </font>
                                    <a href="aaai_paper.pdf" style="color:#800000">[pdf]</a> <a href="http://dblp.uni-trier.de/rec/bibtex/conf/aaai/ZhangEEJ13" style="color:#800000">[bibtex]</a>
                                    </li>
                        </tr>
                        <tr>
                            <td><img src="giraffe.png" alt="object localization" width="280" height="180"></td>
                            <td><b>Object Localization:</b></br>
                                Object localization and Perceptual Grouping of local features in images using visual and spatial affinity in a transductive semi-supervised learning framework.
                                </br></br>
                                This work was published in:
                                </br>
                                <b>Tarek El-Gaaly</b>, Marwan Torki and Ahmed Elgammal, <b><u>Spatial-Visual Label Propagation for Local Feature Classification</u></b>, ICPR 2014
                                <a href="icpr14_labelpropagation.pdf" style="color:#800000">[pdf]</a> <a href="http://dblp.uni-trier.de/rec/bibtex/conf/icpr/El-GaalyTE14" style="color:#800000">[bibtex]</a>
                            </td>
                        </tr>
                        <tr>
                            <td><img src="images2/srr.jpeg" alt="NASA Centennial Challenge 2013 - Sample Return Challenge" width="280" height="180">
                                </br><img src="images2/aero.png" width="280" height="300"></td>
                            <td><b>NASA Centennial Challenge 2013 - Sample Return Challenge:</b></br>
                                In this project I collaborated with Worcester Polytechnic Institute (WPI) to build a rover for the NASA CEntennial Challenge.
                                Our robot AERO (Autonomous Exploration Rover) can be seen to the left on the starting platform with the team in the background.
                                The website documenting the building of the robot can be seen here:
                                <a href="http://robot.wpi.edu/rover/" target="_blank">Blog</a>
                            </td>
                        </tr>
                        <tr>
                            <td><img src="semantic1.png" alt="Semantic Hashing" width="280" height="220"></td>
                            <td><b>Aerial Vehicle Localization using Semantic Geometric Hashing:</b></br>
                                In this work we used semantic features, such as buildings in aerial views to localize in satellite maps. We built an algorithm to perform geometric hashing on these semantic features. This is a form of large-scale global localization across urban terrain.
                                </br></br>
                                This work was published in:
                                </br>
                                Turgay Senlet, <b>Tarek El-Gaaly</b>, Ahmed Elgammal, <b><u>Hierarchical Semantic Hashing: Visual Localization from Buildings on Maps</u></b>, ICPR 2014 <a href="icpr14_maplocalization.pdf" style="color:#800000">[pdf]</a> <a href="ICPR14_MapLocalization_supp.pdf" style="color:#800000">[supplementary material]</a> <a href="http://dblp.uni-trier.de/rec/bibtex/conf/icpr/SenletEE14" style="color:#800000">[bibtex]</a>
                            
                            </td>
                        </tr>
                        <tr>
                            <td><iframe width="280" height="180" src="http://www.youtube.com/embed/ODyHOs0Obqk" allowfullscreen></iframe></td>
                            <td>Experiments in using Micro-Aerial Vehicles for 3D Computer Vision. More videos can be found here: <a title="Aerial Vehicle Research" href="mav.html">Aerial Vehicle research</a></td>
                        </tr>
                        <tr>
                            <td><iframe width="280" height="180" src="http://www.youtube.com/embed/sVeYu3NA8KM" frameborder="0" allowfullscreen></iframe></td>
                            <td><b>Autonomous Airboat Obstacle Avoidance</b>
                                </br>Using monocular vision from an Android smartphone camera, I built an autonomous obstacle avoidance using optical flow, flow trajectory clustering and reflection detection. A live demo is shown on the left. The website for this project is: <a href="http://code.google.com/p/crw-cmu/" target="_blank">(CMU Cooperative Robotic Watercraft)</a>. More videos on <a href="http://robots.net/article/3445.html" target="_blank">Robotics.net</a>.
                                    </br></br>
                                    This work was published in: </br><b>Tarek El-Gaaly</b>, Christopher Tomaszewski, Abhinav Valada et al., <b><u>Visual Obstacle Avoidance for Autonomous Watercraft using Smartphones</b></u>,
                                    Autonomous Robots and Multirobot Systems (ARMS), Workshop in AAMAS 2013 <a href="ARMS2013_VOO_submission.pdf" style="color:#800000">[pdf]</a>
                            </b></td>
                        </tr>
                        <tr>
                            <td><img src="rgbd12.jpg" width="280" height="180" />
                                <img src="rgbd2.jpg" width="280" height="160" />
                                </br></td>
                                <td><b>RGBD Table-top Object Pose Recognition: </b></br>
                                    The red annotations are the ground-truth pose angles (i.e. azimuth/yaw) of the tabletop objects (from the RGBD-dataset - University of Washington). Blue annotations signify the estimated pose based on visual local feature information alone. Green annotations represent the final recognized pose using both visual and depth information.
                                    </br></br>
                                    This work was published in: </br><b>Tarek El-Gaaly</b>, Marwan Torki, Ahmed Elgammal, Maneesh Singh, <b><u>RGBD Object Pose Recognition using Local-Global Multi-Kernel Regression</u></b>, International Conference on Pattern Recognition, ICPR 2012 <a href="icpr_paper.pdf" style="color:#800000">[pdf]</a> <a href="http://dblp.uni-trier.de/rec/bibtex/conf/icpr/El-GaalyT12" style="color:#800000">[bibtex]</a></td>
                        </tr>
                        <tr>
                            <td><iframe width="280" height="180" src="http://www.youtube.com/embed/uk3tunOTgc0?rel=0" frameborder="0" allowfullscreen></iframe></td>
                            <td><b>Autonomous indoor robot navigation: </b>
                            </br>In this work I built an algorithm to perform autonomous indoor navigation using an XboX Kinect sensor on a Pioneer robot (P3DX).</td>
                        </tr>
                        <tr>
                            <td><img src="pollution_spectrum.png" width="280" height="100" /><br/>
                                <img src="pol_dc_co_comparison.png" width="280" height="120" /></td>
                            <td><b>MSc in Computer Science Thesis: </b>In my masters thesis I researched a new method of measuring Atmospheric Scattering from sequences of images. The goal was to correlate these measurements with Particulate Matter (PM10). A byprodut of our approach is image/scene dehazing as seen in the bottom image. The first figure shows a sequence of images of a hazy scene. The second figure shows the scene resulting from 2 state-of-the-art dehazing methods and our dehazing algorithm (rightmost). Our dehazing method recovers the hue of the scene and also returns a natural looking sky without any extra processing.
                                </br></br>
                                This work was published in the International Conference on Computer Vision Theory and Applications (VISAPP) 2010:
                                <br/><br/>
                                <b>Tarek El-Gaaly</b> and Joshua Gluckman, <b><u> Measueing Atmospheric Scattering from Digital Image Sequences</u></b>, VISAPP 2010 <a href="visapp_paper.pdf" style="color:#800000">[pdf]</a> <a href="http://dblp.uni-trier.de/rec/bibtex/conf/visapp/El-GaalyG10" style="color:#800000">[bibtex]</a> </p>
                                </br>
                                <p>For a full description of my MSc Thesis - refer to the thesis document:  <a href="Tarek El-Gaaly - Final Thesis Document.pdf" style="color:#800000">[pdf]</a></td>
                        </tr>
                          <!--
                        <tr>
                            <td><iframe width="280" height="180" src="http://www.youtube.com/embed/IzaWJTiGmww" allowfullscreen></iframe></td>
                            <td><b>RGBD Object Classification</b></td>
                        </tr>-->
                        
                       
                    </table>
                    
                    
                    <!--<li><b>3D Object Recognition</b></li>
                    
                     Perceptually-grounded object-part decomposition of 3D point clouds. We built a Bayesian hierarchical grouping approach for perceptual object-part decomposition based on medial-axis representations of parts. Accepted in AAAI 2015.
                    
                    <br/><img src="3DObjecDecomp2.png" width="480" height="280" /><br/>
                    
                    
                    <br/>-->
                    
                    <!--<li><b>Joint Object Categorization and Pose Estimation</b></li>
                    Using manifold analysis to perform joint object categorization, instance recognition and pose estimation. Multiiple images of an object are known to lie on a low-dimensional view-manifolds. The premise of this work is that feature spaces deform unit circle view-manifolds, in the case of table-top objects rotating on a turn-table and captured by a camera from a fixed height, or a sphere manifold in the more general case. The deformation is captured by a homeomorphic mapping from input feature space to points on a conceptual view-manifold (which can be seen to represent the low-dimensional geometry of views around an object).
                    
                    <br/><img src="joint.jpg" width="480" height="280" /><br/>
                    <img src="joint2.jpg" width="480" height="420" /><br/>



                    </ul>
                    
                    <p></p>
                    <p></p>
                    <p></p>
                    </br>
                    <hr>
                    <hr>
                    </br>
                    
                    <h1>Previous Projects</h1>
                    
                    <p></p>-->
                    
                    <!--<li><b>Quadrocopter Vision for Navigation and Modeling</b></li>
                    <iframe width="480" height="315" src="http://www.youtube.com/embed/ODyHOs0Obqk" frameborder="0" allowfullscreen></iframe>
                    <p></p>
                    More videos can be found here: <a title="Aerial Vehicle Research" href="mav.html">Aerial Vehicle research</a>
                    -->
                    
                    <!--<li><b>NASA Centennial Challenge 2013 - Sample Return Challenge</b></li>
                    Collaborated with Worcester Polytechnic Institute (WPI) on this challenge.
                    Our robot AERO (Autonomous Exploration Rover) can be seen below on the starting platform with us in the background.
                    The website documenting the building of the robot can be seen here:
                    <a href="http://robot.wpi.edu/rover/" target="_blank">Blog</a>
                    <img src="images2/srr.jpeg" alt="NASA Centennial Challenge 2013 - Sample Return Challenge" width="480" height="315">
                        <p></p>
                        <hr>
                     
                        
                        <p></p>
                        <li><b>RGBD Object Classification</b></li>
                        <iframe width="480" height="315" src="http://www.youtube.com/embed/IzaWJTiGmww" frameborder="0" allowfullscreen></iframe>
                        <p></p>
					-->
                      <!--  <hr>
                        </br>
                        <li>
                            <b>Autonomous Airboat Obstacle Avoidance (using monocular vision on a smartphone)</b>
                            </br>Work was conducted at Carnegie Mellon University - Robotics Institute (Field Robotics Center) <a href="http://code.google.com/p/crw-cmu/" target="_blank">(CMU Cooperative Robotic Watercraft)</a>
                            
                        </li>
                        <p></p>-->
                       <!-- More videos on <a href="http://robots.net/article/3445.html" target="_blank">Robotics.net</a>
                        <iframe width="480" height="315" src="http://www.youtube.com/embed/sVeYu3NA8KM" frameborder="0" allowfullscreen></iframe>
                        
                        </br>
                        </br>
                        
                        <hr>
                        
                        <p></p>-->
                        
                        <!--<li><b>RGBD Table-top Object Pose Recognition</b>
                            
                            </br>
                            
                            
                            <img src="rgbd12.jpg" width="480" height="260" />
                            <img src="rgbd2.jpg" width="480" height="170" />
                            </br>
                            
                            
                            The red annotations above are the ground-truth pose angles (i.e. azimuth/yaw) of the tabletop objects
                            
                            (from the RGBD-dataset - University of Washington). Blue annotations signify the estimated pose based on visual local feature information alone. Green annotations represent the final recognized pose using both visual and depth information [Refer to ICPR 2012].
                            <p></p>
                        </li>-->
                        
                     <!--   <hr>
                        </br>
                        <li><b>Autonomous indoor robot navigation and obstacle avoidance (using Kinect)</b></li>
                        
                        <iframe width="480" height="315" src="http://www.youtube.com/embed/uk3tunOTgc0?rel=0" frameborder="0" allowfullscreen></iframe>
                        
                        </br>
                        </br>
                        
                        <hr>
                        -->
                        <!--<p></p>
                        <li><b>Recognizing human activity in video surveillance</b></li>
                        
                        
                        
                        <p></p>-->
                        <!--
                        <li><b>MSc in Computer Science Thesis:</b></br>
                            
                            <ul><li>Measuring atmospheric scattering from sequences of digital images</li>
                                
                                <li>Image/Scene dehazing (images shown below)</li>
                                
                                <li>A framework for particulate matter measurement from atmospheric scattering</li>
                                
                            </ul>
                            
                            
                            <img src="pollution_spectrum.png" width="480" height="140" /><br/>
                            <!--<img src="dehazing.png" width="280" height="140" />
                            <img src="pol_dc_co_comparison.png" width="480" height="140" /><br/>
                            
                       -->
                        
                            </p>
                            <hr>	
                            <hr>	
                            <p></p>
                            <p></p>
                            <p></p>
                            <h4>Funds/Grants</h4>
                            <p></p>
                            <ul>
                                <li>Collaboration and partial funding from Siemens Corporate Research</li>
                                <li>Rutgers faculty seed funding - for multiple UAV vision research</li> 		
                            </ul>
                            <p></p>
                            <p></p>
                            <p></p>
                            <hr>	
                            <hr>	
                            </br>
                            <h1>Teaching</h1>
                            <ul>
                                
                                <li><a href="cs344.html" title="SPRING 2012: CS344 - Design & Analysis of Computer Algorithms" style="color:#000000">SPRING 2012: CS344 - Design & Analysis of Computer Algorithms</a></li>				
                                    <li>FALL 2011: CS205 - Discrete Structures</li>
                                    <li>SPRING 2011: CS334 - Digital imaging and Multimedia</li>
                                    <li>FALL 2010: CS440 - Intro to Artificial Intelligence</li>				
                                    <li>FALL/SPRING 2009-2010: CS110 - Intro to Computers and Applications</li>
                                    
                                    </ul>
                                    
                                    </div>
                                    
                                    
                                    
                                    <div id="mainright">
                                    
                                    <h3>About Me</h3>
                                    <img class="imgright" alt="image description" src="images2/prof.jpg" width="120" height="190" />
                                    <p></p>
                                    <p class="blockquote">I am a Computer Science Ph.D. student @ Rutgers University. I did my undergraduate and masters degrees in computer science at the American University in Cairo.</p>
                                    <p></p>
                                    
                                    <h3>Research Interests</h3>
                                    <p class="blockquote">Computer Vision</br>
                                    Machine Learning</br>
                                    Robotics</br>
                                    AI</br>
                                    Space Exploration</br>
                                    Swarm Intelligence</p>
                                    
                                    <h3>Contact</h3>
                                    <p class="blockquote">tgaaly at cs.rutgers.edu</p>
                                    
                                    
                                    </div>
                                    <div class="clear"></div>
                                    </div>
                                    
                                    <div id="footercont">
                                    <ul>
                                    </ul>
                                    </div>
                                    
                                    </div>
                                    
                                    </body>
                                
